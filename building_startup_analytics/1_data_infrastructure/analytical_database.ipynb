{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7696b25",
   "metadata": {},
   "source": [
    "# Building Analytical Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b733b9e",
   "metadata": {},
   "source": [
    "**Author:**  \n",
    "\n",
    "Pavel Grigoryev\n",
    "\n",
    "**Project Description:**\n",
    "\n",
    "Our startup's messenger and news feed app generates extensive user interaction data. To enable product monitoring, we need to build an optimized analytical database that will power our business intelligence dashboards.\n",
    "\n",
    "**Project Goal:**\n",
    "\n",
    "To design and implement a performant database structure that serves as the backend for product dashboards, enabling tracking of key metrics like DAU, retention, and engagement across both messaging and feed features.\n",
    "\n",
    "**Data Sources:**\n",
    "\n",
    "- `feed_actions` - News feed activity\n",
    "- `message_actions` - Messaging activity \n",
    "\n",
    "**Main Conclusion:**\n",
    "\n",
    "- **Star Schema Designed:** Developed a star schema for the analytical database\n",
    "- **Metric Queries Created:** Built queries to the product database for calculating required metrics for the analytical database\n",
    "- **Star Schema Tables Implemented:** Created all necessary tables for the star schema in the analytical database\n",
    "- **Data Migration Completed:** Extracted required data from the product database and loaded it into the analytical database\n",
    "- **Dashboard Optimization:** Created materialized views to optimize dashboard performance\n",
    "- **Automation Pipeline Built:** Developed Airflow DAG for daily incremental data loading for yesterday's data\n",
    "- **Goal Achieved:** Established analytical database and ETL process for continuous data updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803db27c",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf3b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "import os\n",
    "import itertools\n",
    "from clickhouse_driver import Client\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "load_dotenv();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd237b",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12551883",
   "metadata": {},
   "source": [
    "In the product database on ClickHouse, the data is stored in the following tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20401375",
   "metadata": {},
   "source": [
    "Table feed_actions\n",
    "\n",
    "Field | Description\n",
    "-|-\n",
    "user_id | User ID\n",
    "post_id | Post ID\n",
    "action | Action: view or like\n",
    "time | Timestamp\n",
    "gender | User's gender\n",
    "age | User's age (1 = Male)\n",
    "os | User's OS\n",
    "source | Traffic source\n",
    "country | User's country\n",
    "city | User's city\n",
    "exp_group | A/B test group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75097bf4",
   "metadata": {},
   "source": [
    "Table message_actions\n",
    "\n",
    "Field | Description\n",
    "-|-\n",
    "user_id | Sender's ID\n",
    "receiver_id | Receiver's ID\n",
    "time | Send timestamp\n",
    "gender | Sender's gender\n",
    "age | Sender's age (1 = Male)\n",
    "os | Sender's OS\n",
    "source | Sender's traffic source\n",
    "country | Sender's country\n",
    "city | Sender's city\n",
    "exp_group | Sender's A/B test group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04cab2f",
   "metadata": {},
   "source": [
    "# Database Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468cff48",
   "metadata": {},
   "source": [
    "Let's create clients for connection to source and destination databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601ff41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ch_client = Client(\n",
    "    host=os.getenv('SRC_HOST'),\n",
    "    user=os.getenv('SRC_USER'),\n",
    "    password=os.getenv('SRC_PASSWORD'), \n",
    "    database=os.getenv('SRC_DATABASE'),   \n",
    "    compression=False, \n",
    ")\n",
    "\n",
    "dst_ch_client = Client(\n",
    "    host=os.getenv('DST_HOST'),\n",
    "    user=os.getenv('DST_USER'),\n",
    "    password=os.getenv('DST_PASSWORD'), \n",
    "    database=os.getenv('DST_DATABASE'),  \n",
    "    compression=False,     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba31b70",
   "metadata": {},
   "source": [
    "Create a function that will migrate data from one database to another in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb849b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ch_migrate_db_chunked(\n",
    "    src_client,\n",
    "    dst_client,\n",
    "    src_query,\n",
    "    dst_table_name,\n",
    "    chunk_size=50_000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Migrates data from source to destination in chunks\n",
    "    \n",
    "    Args:\n",
    "        src_client: clickhouse_driver.Client for source\n",
    "        dst_client: clickhouse_driver.Client for destination  \n",
    "        src_query (str): SQL query to source\n",
    "        dst_table_name (str): Table name in destination\n",
    "        chunk_size (int): Chunk size for data migration\n",
    "    \"\"\"\n",
    "    \n",
    "    total_rows = 0\n",
    "    chunk_number = 0\n",
    "    \n",
    "    print(f\"üöÄ Starting data migration to table: {dst_table_name}\")\n",
    "    print(f\"üìä Chunk size: {chunk_size:,} rows\")\n",
    "    \n",
    "    try:\n",
    "        # Determine total row count\n",
    "        print(f\"üìà Calculating total rows for migration...\")     \n",
    "        count_query = f\"SELECT count() FROM ({src_query})\"\n",
    "        total_count = src_client.execute(count_query)[0][0]\n",
    "        print(f\"üìà Total rows to migrate: {total_count:,}\")        \n",
    "        # Use execute_iter for streaming data reading\n",
    "        result_iter = src_client.execute_iter(\n",
    "            src_query, \n",
    "            with_column_types=False  \n",
    "        )    \n",
    "        progress_bar = tqdm(\n",
    "            total=total_count,\n",
    "            desc=\"üì¶ Data migration\",\n",
    "            unit=\" rows\",\n",
    "            unit_scale=True,\n",
    "            ncols=100,\n",
    "            mininterval=1.0, \n",
    "            maxinterval=5.0             \n",
    "        )      \n",
    "        \n",
    "        while True:\n",
    "            # Read chunk_size rows per operation\n",
    "            # Creates iterator slice as another iterator\n",
    "            chunk = list(itertools.islice(result_iter, chunk_size))\n",
    "            # If chunk is empty - exit loop\n",
    "            if not chunk:\n",
    "                break            \n",
    "            chunk_number += 1\n",
    "            dst_client.execute(\n",
    "                f\"INSERT INTO {dst_table_name} VALUES\",\n",
    "                chunk,\n",
    "                types_check=True\n",
    "            )\n",
    "            \n",
    "            total_rows += len(chunk)\n",
    "            progress_bar.update(len(chunk))\n",
    "            progress_bar.set_postfix({\n",
    "                'chunk': chunk_number,\n",
    "                'total': f\"{total_rows:,}\"\n",
    "            })\n",
    "            \n",
    "        progress_bar.close()\n",
    "        print(f\"üéâ Migration completed!\\nTotal migrated: {total_rows:,} rows\\nChunks: {chunk_number}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error migrating table {dst_table_name}: {e}\")\n",
    "        if 'progress_bar' in locals():\n",
    "            progress_bar.close()                    \n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cbd333",
   "metadata": {},
   "source": [
    "# Designing the Analytical Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5634923d",
   "metadata": {},
   "source": [
    "Let's create an analytical database for the dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b593fd",
   "metadata": {},
   "source": [
    "We will use a star schema as it is optimally suited for analytical databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0f25ad",
   "metadata": {},
   "source": [
    "## Creating fact_daily_feed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e409712",
   "metadata": {},
   "source": [
    "Create a fact table for news feed metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdfade5",
   "metadata": {},
   "source": [
    "Create a query to the product database to calculate the required metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f30275",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FACT_DAILY_FEED = \"\"\"\n",
    "    WITH user_actions as (\n",
    "        SELECT\n",
    "            user_id\n",
    "            , post_id\n",
    "            , action\n",
    "            , time\n",
    "            , lagInFrame(time) OVER (PARTITION BY  user_id, toDate(time), action ORDER BY time) prev_time\n",
    "            , minIf(time, action = 'view') OVER (PARTITION BY  user_id, toDate(time), post_id) first_view_in_day\n",
    "            , minIf(time, action = 'like') OVER (PARTITION BY  user_id, toDate(time), post_id) first_like_in_day\n",
    "        FROM \n",
    "            feed_actions \n",
    "        WHERE \n",
    "            toDate(time) <= yesterday()  \n",
    "    )\n",
    "    SELECT\n",
    "        toDate(time) as date\n",
    "        , user_id\n",
    "        , minIf(time, action = 'view') as first_view_time\n",
    "        , minIf(time, action = 'like') as first_like_time\n",
    "        , maxIf(time, action = 'view') as last_view_time\n",
    "        , maxIf(time, action = 'like') as last_like_time    \n",
    "        , uniq(post_id) as posts\n",
    "        , countIf(action = 'view') as views\n",
    "        , countIf(action = 'like') as likes\n",
    "        , avgIf(time - prev_time, action = 'view' and prev_time != toDateTime(0)) as avg_time_between_views\n",
    "        , avgIf(time - prev_time, action = 'like' and prev_time != toDateTime(0)) as avg_time_between_likes\n",
    "        , avgIf(\n",
    "            first_like_in_day - first_view_in_day\n",
    "            , first_like_in_day != toDateTime(0) \n",
    "            and first_view_in_day != toDateTime(0) \n",
    "            and first_like_in_day >= first_view_in_day\n",
    "        ) as avg_view_to_like_seconds    \n",
    "    FROM \n",
    "        user_actions\n",
    "    GROUP BY\n",
    "        date\n",
    "        , user_id\n",
    "    ORDER BY \n",
    "        date\n",
    "        , user_id     \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89443191",
   "metadata": {},
   "source": [
    "Create a table in the analytical database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f088d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_create = \"\"\"\n",
    "    CREATE TABLE fact_daily_feed (\n",
    "        date Date\n",
    "        , user_id UInt32\n",
    "        , first_view_time DateTime\n",
    "        , first_like_time DateTime\n",
    "        , last_view_time DateTime\n",
    "        , last_like_time DateTime        \n",
    "        , posts UInt16\n",
    "        , views UInt16\n",
    "        , likes UInt16\n",
    "        , avg_time_between_views Float32\n",
    "        , avg_time_between_likes Float32\n",
    "        , avg_view_to_like_seconds Float32\n",
    "    ) ENGINE = MergeTree()\n",
    "    PARTITION BY toYYYYMM(date)\n",
    "    ORDER BY (date, user_id, views, likes)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bdd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_ch_client.execute(query_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8b019",
   "metadata": {},
   "source": [
    "Populate the table with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_migrate_db_chunked(\n",
    "    src_client=src_ch_client,\n",
    "    dst_client=dst_ch_client,\n",
    "    src_query=QUERY_FACT_DAILY_FEED,\n",
    "    dst_table_name='fact_daily_feed',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25038937",
   "metadata": {},
   "source": [
    "## Creating fact_daily_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d9d69",
   "metadata": {},
   "source": [
    "Create a fact table for post metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a2815",
   "metadata": {},
   "source": [
    "Create a query to the product database to calculate the required metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FACT_DAILY_POSTS = \"\"\"\n",
    "    WITH post_actions as (\n",
    "        SELECT\n",
    "            user_id\n",
    "            , post_id\n",
    "            , action\n",
    "            , time\n",
    "            , lagInFrame(time) OVER (PARTITION BY  post_id, toDate(time), action ORDER BY time) prev_time\n",
    "            , minIf(time, action = 'view') OVER (PARTITION BY toDate(time), post_id) first_view_in_day\n",
    "            , minIf(time, action = 'like') OVER (PARTITION BY toDate(time), post_id) first_like_in_day\n",
    "        FROM \n",
    "            feed_actions \n",
    "        WHERE \n",
    "            toDate(time) <= yesterday()    \n",
    "    )\n",
    "    SELECT\n",
    "        toDate(time) as date\n",
    "        , post_id\n",
    "        , min(time) as first_action_time\n",
    "        , max(time) as last_action_time\n",
    "        , countIf(action = 'view') as views\n",
    "        , countIf(action = 'like') as likes\n",
    "        , uniqIf(user_id, action = 'view') as unique_viewers\n",
    "        , uniqIf(user_id, action = 'like') as unique_likers\n",
    "        , avgIf(time - prev_time, action = 'view' and prev_time != toDateTime(0)) as avg_time_between_views\n",
    "        , avgIf(time - prev_time, action = 'like' and prev_time != toDateTime(0)) as avg_time_between_likes\n",
    "        , avgIf(\n",
    "            first_like_in_day - first_view_in_day\n",
    "            , first_like_in_day != toDateTime(0) \n",
    "            and first_view_in_day != toDateTime(0) \n",
    "            and first_like_in_day >= first_view_in_day\n",
    "        ) as avg_view_to_like_seconds    \n",
    "    FROM \n",
    "        post_actions\n",
    "    GROUP BY\n",
    "        date\n",
    "        , post_id\n",
    "    ORDER BY \n",
    "        date\n",
    "        , post_id     \n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a2d0c",
   "metadata": {},
   "source": [
    "Create a table in the analytical database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_create = \"\"\"\n",
    "    CREATE TABLE fact_daily_posts (\n",
    "        date Date\n",
    "        , post_id UInt32\n",
    "        , first_action_time DateTime\n",
    "        , last_action_time DateTime\n",
    "        , views UInt32\n",
    "        , likes UInt16\n",
    "        , unique_viewers UInt16\n",
    "        , unique_likers UInt16\n",
    "        , avg_time_between_views Float32\n",
    "        , avg_time_between_likes Float32\n",
    "        , avg_view_to_like_seconds Float32\n",
    "    ) ENGINE = MergeTree()\n",
    "    PARTITION BY toYYYYMM(date)\n",
    "    ORDER BY (date, post_id, views, likes)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df13699",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_ch_client.execute(query_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2a414",
   "metadata": {},
   "source": [
    "Populate the table with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057dd92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_migrate_db_chunked(\n",
    "    src_client=src_ch_client,\n",
    "    dst_client=dst_ch_client,\n",
    "    src_query=QUERY_FACT_DAILY_POSTS,\n",
    "    dst_table_name='fact_daily_posts',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf484f50",
   "metadata": {},
   "source": [
    "## Creating fact_daily_messenger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a99dae7",
   "metadata": {},
   "source": [
    "Create a fact table for messenger metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0730ee2",
   "metadata": {},
   "source": [
    "Create a query to the product database to calculate the required metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82882b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FACT_DAILY_MESSENGER = \"\"\"\n",
    "    WITH user_actions as (\n",
    "        SELECT \n",
    "            time\n",
    "            , user_id\n",
    "            , receiver_id \n",
    "            , lagInFrame(time) OVER (PARTITION BY  toDate(time), user_id ORDER BY time) user_prev_time\n",
    "            , lagInFrame(time) OVER (PARTITION BY  toDate(time), receiver_id ORDER BY time) receiver_prev_time\n",
    "        FROM \n",
    "            message_actions \n",
    "        WHERE \n",
    "            toDate(time) <= yesterday()    \n",
    "        )\n",
    "        , senders as (\n",
    "        SELECT\n",
    "            toDate(time) as date\n",
    "            , user_id \n",
    "            , min(time) as first_sent_time\n",
    "            , max(time) as last_sent_time    \n",
    "            , uniq(receiver_id) as users_sent\n",
    "            , count() as messages_sent\n",
    "            , avgIf(time - user_prev_time, user_prev_time != toDateTime(0)) avg_time_between_messages_sent\n",
    "        FROM \n",
    "            user_actions   \n",
    "        GROUP BY \n",
    "            date\n",
    "            , user_id\n",
    "        )\n",
    "    , receivers as (\n",
    "        SELECT\n",
    "            toDate(time) as date\n",
    "            , receiver_id as user_id \n",
    "            , min(time) as first_received_time\n",
    "            , max(time) as last_received_time      \n",
    "            , uniq(user_id) as users_received\n",
    "            , count() as messages_received\n",
    "            , avgIf(time - receiver_prev_time, receiver_prev_time != toDateTime(0)) avg_time_between_messages_received\n",
    "        FROM \n",
    "            user_actions  \n",
    "        GROUP BY \n",
    "            date\n",
    "            , receiver_id\n",
    "    )\n",
    "    SELECT \n",
    "        if(s.user_id != 0, s.date, r.date) as date\n",
    "        , if(s.user_id != 0, s.user_id, r.user_id) as user_id\n",
    "        , s.first_sent_time\n",
    "        , s.last_sent_time\n",
    "        , s.users_sent\n",
    "        , s.messages_sent\n",
    "        , s.avg_time_between_messages_sent\n",
    "        , r.first_received_time\n",
    "        , r.last_received_time\n",
    "        , r.users_received\n",
    "        , r.messages_received\n",
    "        , r.avg_time_between_messages_received  \n",
    "    FROM \n",
    "        senders s\n",
    "        FULL JOIN receivers r ON s.date = r.date AND s.user_id = r.user_id\n",
    "    ORDER BY \n",
    "        date\n",
    "        , user_id      \n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e2ccc",
   "metadata": {},
   "source": [
    "Create a table in the analytical database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_create = \"\"\"\n",
    "    CREATE TABLE fact_daily_messenger (\n",
    "        date Date\n",
    "        , user_id UInt32\n",
    "        , first_sent_time DateTime\n",
    "        , last_sent_time DateTime\n",
    "        , users_sent UInt16\n",
    "        , messages_sent UInt16\n",
    "        , avg_time_between_messages_sent Float32\n",
    "        , first_received_time DateTime\n",
    "        , last_received_time DateTime\n",
    "        , users_received UInt16\n",
    "        , messages_received UInt16\n",
    "        , avg_time_between_messages_received Float32\n",
    "    ) ENGINE = MergeTree()\n",
    "    PARTITION BY toYYYYMM(date)\n",
    "    ORDER BY (date, user_id, messages_sent, messages_received)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_ch_client.execute(query_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e84843",
   "metadata": {},
   "source": [
    "Populate the table with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650af448",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_migrate_db_chunked(\n",
    "    src_client=src_ch_client,\n",
    "    dst_client=dst_ch_client,\n",
    "    src_query=QUERY_FACT_DAILY_MESSENGER,\n",
    "    dst_table_name='fact_daily_messenger',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80408ff",
   "metadata": {},
   "source": [
    "## Creating fact_user_connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2972e3",
   "metadata": {},
   "source": [
    "Create a fact table for user connections in the messenger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b118f23",
   "metadata": {},
   "source": [
    "Create a query to the product database to calculate the required metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b78bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FACT_USER_CONNECTIONS = \"\"\"\n",
    "    SELECT \n",
    "        toDate(time) as date\n",
    "        , user_id as sender_id\n",
    "        , receiver_id\n",
    "        , count() as messages_count\n",
    "    FROM \n",
    "        message_actions\n",
    "    WHERE \n",
    "        toDate(time) <= yesterday()  \n",
    "    GROUP BY \n",
    "        date\n",
    "        , sender_id\n",
    "        , receiver_id\n",
    "    ORDER BY \n",
    "        date\n",
    "        , sender_id\n",
    "        , receiver_id     \n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1109ab7",
   "metadata": {},
   "source": [
    "Create a table in the analytical database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe465d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_create = \"\"\"\n",
    "    CREATE TABLE fact_user_connections (\n",
    "        date Date\n",
    "        , sender_id UInt32\n",
    "        , receiver_id UInt32\n",
    "        , messages_count UInt16\n",
    "    ) ENGINE = MergeTree()\n",
    "    PARTITION BY toYYYYMM(date)\n",
    "    ORDER BY (date, sender_id, receiver_id, messages_count)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd83da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_ch_client.execute(query_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a1477",
   "metadata": {},
   "source": [
    "Populate the table with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0cc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_migrate_db_chunked(\n",
    "    src_client=src_ch_client,\n",
    "    dst_client=dst_ch_client,\n",
    "    src_query=QUERY_FACT_USER_CONNECTIONS,\n",
    "    dst_table_name='fact_user_connections',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db16ca7",
   "metadata": {},
   "source": [
    "## Creating dim_users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c3c8d7",
   "metadata": {},
   "source": [
    "Create a dimension table for users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca80bb58",
   "metadata": {},
   "source": [
    "Create a query to the product database to calculate the required metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c764b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_DIM_USERS = \"\"\"\n",
    "    SELECT \n",
    "        user_id\n",
    "        , argMax(gender, time) as gender\n",
    "        , argMax(age, time) as age\n",
    "        , argMax(source, time) as source\n",
    "        , argMax(os, time) as os\n",
    "        , argMax(city, time) as city\n",
    "        , argMax(country, time) as country\n",
    "        , argMax(exp_group, time) as exp_group\n",
    "        , toDate(now()) as version\n",
    "    FROM (\n",
    "        SELECT\n",
    "            time\n",
    "            , user_id\n",
    "            , gender\n",
    "            , age\n",
    "            , source\n",
    "            , os \n",
    "            , city \n",
    "            , country\n",
    "            , exp_group \n",
    "        FROM \n",
    "            feed_actions \n",
    "        UNION ALL\n",
    "        SELECT\n",
    "            time\n",
    "            , user_id\n",
    "            , gender\n",
    "            , age\n",
    "            , source\n",
    "            , os \n",
    "            , city \n",
    "            , country\n",
    "            , exp_group       \n",
    "        FROM \n",
    "            message_actions    \n",
    "    )\n",
    "    WHERE \n",
    "        toDate(time) <= yesterday()\n",
    "    GROUP BY \n",
    "        user_id\n",
    "    ORDER BY \n",
    "        user_id\n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4267d",
   "metadata": {},
   "source": [
    "Create a table in the analytical database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec687221",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_create = \"\"\"\n",
    "    CREATE TABLE dim_users (\n",
    "        user_id UInt32\n",
    "        , gender UInt8\n",
    "        , age UInt8\n",
    "        , source LowCardinality(String)\n",
    "        , os LowCardinality(String)\n",
    "        , city String\n",
    "        , country LowCardinality(String)\n",
    "        , exp_group UInt8\n",
    "        , version Date  \n",
    "    ) ENGINE = ReplacingMergeTree(version)\n",
    "    ORDER BY (user_id)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951cefa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_ch_client.execute(query_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f50ec",
   "metadata": {},
   "source": [
    "Populate the table with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88058cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_migrate_db_chunked(\n",
    "    src_client=src_ch_client,\n",
    "    dst_client=dst_ch_client,\n",
    "    src_query=QUERY_DIM_USERS,\n",
    "    dst_table_name='dim_users',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a2c62",
   "metadata": {},
   "source": [
    "## Creating dim_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf973c68",
   "metadata": {},
   "source": [
    "Create a dimension table for dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_create = \"\"\"\n",
    "    CREATE TABLE dim_dates (\n",
    "        date Date\n",
    "        , day_of_week UInt8\n",
    "        , day_name String\n",
    "        , month UInt8\n",
    "        , month_name String\n",
    "        , quarter UInt8\n",
    "        , year UInt16\n",
    "        , is_weekend UInt8\n",
    "        , week_number UInt8\n",
    "    ) ENGINE = MergeTree()\n",
    "    ORDER BY (date)\n",
    "    PRIMARY KEY (date);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dace6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_ch_client.execute(query_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a1db2",
   "metadata": {},
   "source": [
    "Populate the table with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_insert = \"\"\"\n",
    "    INSERT INTO dim_dates\n",
    "    SELECT \n",
    "        date\n",
    "        , toDayOfWeek(date) as day_of_week\n",
    "        , dateName('day', date) as day_name\n",
    "        , toMonth(date) as month\n",
    "        , dateName('month', date) as month_name\n",
    "        , toQuarter(date) as quarter\n",
    "        , toYear(date) as year\n",
    "        , if(day_of_week IN (6, 7), 1, 0) as is_weekend\n",
    "        , toISOWeek(date) as week_number\n",
    "    FROM (\n",
    "        SELECT toDate('2025-01-01') + number as date\n",
    "        FROM numbers(730)  \n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec50160",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_ch_client.execute(query_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a347c41",
   "metadata": {},
   "source": [
    "As a result, we obtained the following schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ccf99c",
   "metadata": {},
   "source": [
    "<img src=\"er.png\" alt=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4f73e",
   "metadata": {},
   "source": [
    "# Creating Materialized Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d0669",
   "metadata": {},
   "source": [
    "To optimize data loading into the dashboard, let's create materialized views."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6bb29",
   "metadata": {},
   "source": [
    "## App Daily Activity Materialized View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe31457",
   "metadata": {},
   "source": [
    "Create materialized –ºiew for app daily activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_APP_DAILY_ACTIVITY_MV = \"\"\"\n",
    "    CREATE MATERIALIZED VIEW mv_app_daily_activity AS\n",
    "    WITH merged_stats AS ( \n",
    "        SELECT \n",
    "            COALESCE(f.date, m.date) AS date\n",
    "            , COALESCE(f.user_id, m.user_id) AS user_id\n",
    "            , f.likes\n",
    "            , f.views\n",
    "            , m.messages_sent\n",
    "        FROM\n",
    "            fact_daily_feed f\n",
    "            FULL JOIN fact_daily_messenger m ON f.date = m.date AND f.user_id = m.user_id \n",
    "    )\n",
    "    , daily_metrics AS (\n",
    "        SELECT \n",
    "            date\n",
    "            , user_id\n",
    "            , date - MIN(date) OVER (PARTITION BY user_id) AS lifetime\n",
    "            , views\n",
    "            , likes \n",
    "            , posts    \n",
    "            , messages_sent \n",
    "        FROM\n",
    "            merged_stats\n",
    "    )\n",
    "    , base_metrics AS (\n",
    "        SELECT \n",
    "            m.date\n",
    "            , u.gender\n",
    "            , u.age \n",
    "            , u.source\n",
    "            , u.os\n",
    "            , u.city \n",
    "            , u.country\n",
    "            , COUNT(DISTINCT m.user_id) AS total_users\n",
    "            , COUNT(DISTINCT m.user_id) FILTER (WHERE messages_sent IS NULL) AS feed_only_users\n",
    "            , COUNT(DISTINCT m.user_id) FILTER (WHERE views IS NULL) AS messenger_only_users\n",
    "            , COUNT(DISTINCT m.user_id) FILTER (WHERE views IS NOT NULL AND messages_sent IS NOT NULL) AS both_services_users\n",
    "            , COUNT(DISTINCT m.user_id) FILTER (WHERE messages_sent IS NOT NULL) AS messenger_users\n",
    "            , COUNT(DISTINCT m.user_id) FILTER (WHERE lifetime = 0) AS new_users_lifetime_0\n",
    "            , COUNT(DISTINCT m.user_id) FILTER (WHERE lifetime = 7) AS users_lifetime_7d\n",
    "            , SUM(views) AS views\n",
    "            , SUM(likes) AS likes\n",
    "            , SUM(messages_sent) AS messages_sent\n",
    "        FROM\n",
    "            daily_metrics m\n",
    "            LEFT JOIN dim_users u ON m.user_id = u.user_id \n",
    "        GROUP BY\n",
    "            m.date\n",
    "            , u.gender\n",
    "            , u.age \n",
    "            , u.source\n",
    "            , u.os\n",
    "            , u.city \n",
    "            , u.country\n",
    "    )\n",
    "    , lag_metrics AS (\n",
    "        SELECT \n",
    "            date + INTERVAL '7 days' as date\n",
    "            , gender\n",
    "            , age \n",
    "            , source\n",
    "            , os\n",
    "            , city \n",
    "            , country\n",
    "            , new_users_lifetime_0 as new_users_7_days_ago\n",
    "        FROM \n",
    "            base_metrics\n",
    "    )\n",
    "    SELECT \n",
    "        COALESCE(b.date, l.date) as date\n",
    "        , COALESCE(b.gender, l.gender) as gender\n",
    "        , COALESCE(b.age, l.age) as age\n",
    "        , COALESCE(b.source, l.source) as source\n",
    "        , COALESCE(b.os, l.os) as os\n",
    "        , COALESCE(b.city, l.city) as city\n",
    "        , COALESCE(b.country, l.country) as country\n",
    "        , COALESCE(b.total_users, 0) as total_users\n",
    "        , COALESCE(b.feed_only_users, 0) as feed_only_users\n",
    "        , COALESCE(b.messenger_only_users, 0) as messenger_only_users\n",
    "        , COALESCE(b.both_services_users, 0) as both_services_users\n",
    "        , COALESCE(b.messenger_users, 0) as messenger_users\n",
    "        , COALESCE(b.new_users_lifetime_0, 0) as new_users_lifetime_0\n",
    "        , COALESCE(b.users_lifetime_7d, 0) as users_lifetime_7d\n",
    "        , COALESCE(l.new_users_7_days_ago, 0) as new_users_7_days_ago\n",
    "        , COALESCE(b.views, 0) as views\n",
    "        , COALESCE(b.likes, 0) as likes\n",
    "    FROM \n",
    "        base_metrics b\n",
    "        FULL JOIN lag_metrics l ON \n",
    "            b.date = l.date \n",
    "            AND b.gender = l.gender\n",
    "            AND b.age = l.age\n",
    "            AND b.source = l.source\n",
    "            AND b.os = l.os\n",
    "            AND b.city = l.city\n",
    "            AND b.country = l.country\n",
    "    ORDER BY\n",
    "        date\n",
    "        , gender\n",
    "        , age\n",
    "        , source\n",
    "        , os\n",
    "        , city\n",
    "        , country\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41557b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_ch_client.execute(QUERY_APP_DAILY_ACTIVITY_MV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0136ff68",
   "metadata": {},
   "source": [
    "## Feed Daily Activity Materialized View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24490a57",
   "metadata": {},
   "source": [
    "Create materialized –ºiew for feed daily activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5149ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FEED_DAILY_ACTIVITY_MV = \"\"\"\n",
    "    CREATE MATERIALIZED VIEW mv_feed_daily_activity AS\n",
    "    WITH daily_metrics AS (\n",
    "        SELECT \n",
    "            date\n",
    "            , user_id\n",
    "            , date - MIN(date) OVER (PARTITION BY user_id) AS lifetime\n",
    "            , views\n",
    "            , likes \n",
    "            , posts\n",
    "            , avg_view_to_like_seconds        \n",
    "        FROM\n",
    "            fact_daily_feed\n",
    "    )\n",
    "    , base_metrics AS (\n",
    "        SELECT \n",
    "            m.date\n",
    "            , u.gender\n",
    "            , u.age \n",
    "            , u.source\n",
    "            , u.os\n",
    "            , u.city \n",
    "            , u.country\n",
    "            , COUNT(DISTINCT m.user_id) AS feed_users\n",
    "            , COUNT(DISTINCT m.user_id) FILTER (WHERE lifetime = 0) AS feed_new_users_lifetime_0\n",
    "            , COUNT(DISTINCT m.user_id) FILTER (WHERE lifetime = 7) AS feed_users_lifetime_7d\n",
    "            , SUM(views) AS views\n",
    "            , SUM(likes) AS likes\n",
    "            , SUM(posts) AS posts\n",
    "            , AVG(avg_view_to_like_seconds) AS avg_view_to_like_seconds\n",
    "        FROM\n",
    "            daily_metrics m\n",
    "            LEFT JOIN public.dim_users u ON m.user_id = u.user_id \n",
    "        GROUP BY\n",
    "            m.date\n",
    "            , u.gender\n",
    "            , u.age \n",
    "            , u.source\n",
    "            , u.os\n",
    "            , u.city \n",
    "            , u.country\n",
    "    )\n",
    "    , lag_metrics AS (\n",
    "        SELECT \n",
    "            date + INTERVAL '7 days' as date\n",
    "            , gender\n",
    "            , age \n",
    "            , source\n",
    "            , os\n",
    "            , city \n",
    "            , country\n",
    "            , feed_new_users_lifetime_0 as feed_new_users_7_days_ago\n",
    "        FROM \n",
    "            base_metrics\n",
    "    )\n",
    "    SELECT \n",
    "        COALESCE(b.date, l.date) as date\n",
    "        , COALESCE(b.gender, l.gender) as gender\n",
    "        , COALESCE(b.age, l.age) as age\n",
    "        , COALESCE(b.source, l.source) as source\n",
    "        , COALESCE(b.os, l.os) as os\n",
    "        , COALESCE(b.city, l.city) as city\n",
    "        , COALESCE(b.country, l.country) as country\n",
    "        , COALESCE(b.views, 0) as views\n",
    "        , COALESCE(b.likes, 0) as likes\n",
    "        , COALESCE(b.posts, 0) as posts\n",
    "        , COALESCE(b.avg_view_to_like_seconds, 0) as avg_view_to_like_seconds\n",
    "        , COALESCE(b.feed_users, 0) as feed_users\n",
    "        , COALESCE(b.feed_new_users_lifetime_0, 0) as feed_new_users_lifetime_0\n",
    "        , COALESCE(b.feed_users_lifetime_7d, 0) as feed_users_lifetime_7d\n",
    "        , COALESCE(l.feed_new_users_7_days_ago, 0) as feed_new_users_7_days_ago\n",
    "    FROM \n",
    "        base_metrics b\n",
    "        FULL JOIN lag_metrics l ON \n",
    "            b.date = l.date \n",
    "            AND b.gender = l.gender\n",
    "            AND b.age = l.age\n",
    "            AND b.source = l.source\n",
    "            AND b.os = l.os\n",
    "            AND b.city = l.city\n",
    "            AND b.country = l.country\n",
    "    ORDER BY\n",
    "        date\n",
    "        , gender\n",
    "        , age\n",
    "        , source\n",
    "        , os\n",
    "        , city\n",
    "        , country\n",
    "\"\"\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e0279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_ch_client.execute(QUERY_FEED_DAILY_ACTIVITY_MV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc345bd",
   "metadata": {},
   "source": [
    "## Messenger Daily Activity Materialized View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189222d",
   "metadata": {},
   "source": [
    "Create materialized –ºiew for messenger daily activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bcd8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_MESSENGER_DAILY_ACTIVITY_MV = \"\"\"\n",
    "    CREATE MATERIALIZED VIEW mv_messenger_daily_activity AS\n",
    "    WITH daily_metrics AS (\n",
    "        SELECT \n",
    "            date\n",
    "            , user_id\n",
    "            , date - MIN(date) OVER (PARTITION BY user_id) AS lifetime\n",
    "            , messages_sent\n",
    "            , messages_received\n",
    "            , users_sent\n",
    "            , users_received\n",
    "            , avg_time_between_messages_sent\n",
    "        FROM \n",
    "            fact_daily_messenger\n",
    "    )\n",
    "    , base_metrics AS (\n",
    "        SELECT \n",
    "            m.date\n",
    "            , u.gender\n",
    "            , u.age \n",
    "            , u.source\n",
    "            , u.os\n",
    "            , u.city \n",
    "            , u.country\n",
    "            , COUNT(DISTINCT m.user_id) FILTER (WHERE users_sent != 0) AS total_senders\n",
    "            , COUNT(DISTINCT m.user_id) FILTER (WHERE users_sent = 0) AS total_receivers\n",
    "            , COUNT(DISTINCT m.user_id) FILTER (WHERE lifetime = 0) AS new_users_lifetime_0\n",
    "            , COUNT(DISTINCT m.user_id) FILTER (WHERE lifetime = 7) AS users_lifetime_7d        \n",
    "            , SUM(messages_sent) AS messages_sent\n",
    "            , AVG(avg_time_between_messages_sent) AS avg_time_between_messages_sent\n",
    "        FROM \n",
    "            daily_metrics m\n",
    "            LEFT JOIN public.dim_users u ON m.user_id = u.user_id \n",
    "        GROUP BY\n",
    "            m.date\n",
    "            , u.gender\n",
    "            , u.age \n",
    "            , u.source\n",
    "            , u.os\n",
    "            , u.city \n",
    "            , u.country\n",
    "    )\n",
    "    , lag_metrics AS (\n",
    "        SELECT \n",
    "            date + INTERVAL '7 days' as date\n",
    "            , gender\n",
    "            , age \n",
    "            , source\n",
    "            , os\n",
    "            , city \n",
    "            , country\n",
    "            , new_users_lifetime_0 as new_messenger_users_7_days_ago\n",
    "        FROM \n",
    "            base_metrics\n",
    "    )\n",
    "    SELECT \n",
    "        COALESCE(b.date, l.date) as date\n",
    "        , COALESCE(b.gender, l.gender) as gender\n",
    "        , COALESCE(b.age, l.age) as age\n",
    "        , COALESCE(b.source, l.source) as source\n",
    "        , COALESCE(b.os, l.os) as os\n",
    "        , COALESCE(b.city, l.city) as city\n",
    "        , COALESCE(b.country, l.country) as country\n",
    "        , COALESCE(b.total_senders, 0) as total_senders\n",
    "        , COALESCE(b.total_receivers, 0) as total_receivers\n",
    "        , COALESCE(b.new_users_lifetime_0, 0) as new_users_lifetime_0\n",
    "        , COALESCE(b.users_lifetime_7d, 0) as users_lifetime_7d\n",
    "        , COALESCE(l.new_messenger_users_7_days_ago, 0) as new_messenger_users_7_days_ago\n",
    "        , COALESCE(b.messages_sent, 0) as messages_sent\n",
    "        , COALESCE(b.avg_time_between_messages_sent, 0) as avg_time_between_messages_sent\n",
    "    FROM \n",
    "        base_metrics b\n",
    "        FULL JOIN lag_metrics l ON \n",
    "            b.date = l.date \n",
    "            AND b.gender = l.gender\n",
    "            AND b.age = l.age\n",
    "            AND b.source = l.source\n",
    "            AND b.os = l.os\n",
    "            AND b.city = l.city\n",
    "            AND b.country = l.country\n",
    "    ORDER BY\n",
    "        date\n",
    "        , gender\n",
    "        , age\n",
    "        , source\n",
    "        , os\n",
    "        , city\n",
    "        , country\n",
    "\"\"\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac295a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_ch_client.execute(QUERY_MESSENGER_DAILY_ACTIVITY_MV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fef3f1",
   "metadata": {},
   "source": [
    "# ETL Process (Airflow DAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8b0c4",
   "metadata": {},
   "source": [
    "To add data for yesterday, we will create an Airflow DAG.\n",
    "\n",
    "- We will create a file with queries to extract data for yesterday.\n",
    "- The queries for the tables will be analogous to the queries used to create the analytical database. Only the data will be taken for yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analytics_daily_etl_queries.py\n",
    "\"\"\"\n",
    "SQL queries for analytics_daily_etl dag\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================================================\n",
    "# Queries for extract\n",
    "# ==========================================================================\n",
    "\n",
    "QUERY_FACT_DAILY_FEED_EXTRACT = \"\"\"\n",
    "    WITH user_actions as (\n",
    "        SELECT\n",
    "            user_id\n",
    "            , post_id\n",
    "            , action\n",
    "            , time\n",
    "            , lagInFrame(time) OVER (PARTITION BY  user_id, toDate(time), action ORDER BY time) prev_time\n",
    "            , minIf(time, action = 'view') OVER (PARTITION BY  user_id, toDate(time), post_id) first_view_in_day\n",
    "            , minIf(time, action = 'like') OVER (PARTITION BY  user_id, toDate(time), post_id) first_like_in_day\n",
    "        FROM \n",
    "            feed_actions \n",
    "        WHERE \n",
    "            toDate(time) = yesterday()    \n",
    "    )\n",
    "    SELECT\n",
    "        toDate(time) as date\n",
    "        , user_id\n",
    "        , minIf(time, action = 'view') as first_view_time\n",
    "        , minIf(time, action = 'like') as first_like_time\n",
    "        , maxIf(time, action = 'view') as last_view_time\n",
    "        , maxIf(time, action = 'like') as last_like_time    \n",
    "        , uniq(post_id) as posts\n",
    "        , countIf(action = 'view') as views\n",
    "        , countIf(action = 'like') as likes\n",
    "        , avgIf(time - prev_time, action = 'view' and prev_time != toDateTime(0)) as avg_time_between_views\n",
    "        , avgIf(time - prev_time, action = 'like' and prev_time != toDateTime(0)) as avg_time_between_likes\n",
    "        , avgIf(\n",
    "            first_like_in_day - first_view_in_day\n",
    "            , first_like_in_day != toDateTime(0) \n",
    "            and first_view_in_day != toDateTime(0) \n",
    "            and first_like_in_day >= first_view_in_day\n",
    "        ) as avg_view_to_like_seconds    \n",
    "    FROM \n",
    "        user_actions\n",
    "    GROUP BY\n",
    "        date\n",
    "        , user_id\n",
    "    ORDER BY \n",
    "        date\n",
    "        , user_id     \n",
    "\"\"\"\n",
    "\n",
    "QUERY_FACT_DAILY_POSTS_EXTRACT = \"\"\"\n",
    "    WITH post_actions as (\n",
    "        SELECT\n",
    "            user_id\n",
    "            , post_id\n",
    "            , action\n",
    "            , time\n",
    "            , lagInFrame(time) OVER (PARTITION BY  post_id, toDate(time), action ORDER BY time) prev_time\n",
    "            , minIf(time, action = 'view') OVER (PARTITION BY toDate(time), post_id) first_view_in_day\n",
    "            , minIf(time, action = 'like') OVER (PARTITION BY toDate(time), post_id) first_like_in_day\n",
    "        FROM \n",
    "            feed_actions \n",
    "        WHERE \n",
    "            toDate(time) = yesterday()    \n",
    "    )\n",
    "    SELECT\n",
    "        toDate(time) as date\n",
    "        , post_id\n",
    "        , min(time) as first_action_time\n",
    "        , max(time) as last_action_time\n",
    "        , countIf(action = 'view') as views\n",
    "        , countIf(action = 'like') as likes\n",
    "        , uniqIf(user_id, action = 'view') as unique_viewers\n",
    "        , uniqIf(user_id, action = 'like') as unique_likers\n",
    "        , avgIf(time - prev_time, action = 'view' and prev_time != toDateTime(0)) as avg_time_between_views\n",
    "        , avgIf(time - prev_time, action = 'like' and prev_time != toDateTime(0)) as avg_time_between_likes\n",
    "        , avgIf(\n",
    "            first_like_in_day - first_view_in_day\n",
    "            , first_like_in_day != toDateTime(0) \n",
    "            and first_view_in_day != toDateTime(0) \n",
    "            and first_like_in_day >= first_view_in_day\n",
    "        ) as avg_view_to_like_seconds    \n",
    "    FROM \n",
    "        post_actions\n",
    "    GROUP BY\n",
    "        date\n",
    "        , post_id\n",
    "    ORDER BY \n",
    "        date\n",
    "        , post_id           \n",
    "\"\"\"    \n",
    "\n",
    "QUERY_FACT_DAILY_MESSENGER_EXTRACT = \"\"\"\n",
    "    WITH user_actions as (\n",
    "        SELECT \n",
    "            time\n",
    "            , user_id\n",
    "            , receiver_id \n",
    "            , lagInFrame(time) OVER (PARTITION BY  toDate(time), user_id ORDER BY time) user_prev_time\n",
    "            , lagInFrame(time) OVER (PARTITION BY  toDate(time), receiver_id ORDER BY time) receiver_prev_time\n",
    "        FROM \n",
    "            message_actions \n",
    "        WHERE \n",
    "            toDate(time) = yesterday()    \n",
    "        )\n",
    "        , senders as (\n",
    "        SELECT\n",
    "            toDate(time) as date\n",
    "            , user_id \n",
    "            , min(time) as first_sent_time\n",
    "            , max(time) as last_sent_time    \n",
    "            , uniq(receiver_id) as users_sent\n",
    "            , count() as messages_sent\n",
    "            , avgIf(time - user_prev_time, user_prev_time != toDateTime(0)) avg_time_between_messages_sent\n",
    "        FROM \n",
    "            user_actions   \n",
    "        GROUP BY \n",
    "            date\n",
    "            , user_id\n",
    "        )\n",
    "    , receivers as (\n",
    "        SELECT\n",
    "            toDate(time) as date\n",
    "            , receiver_id as user_id \n",
    "            , min(time) as first_received_time\n",
    "            , max(time) as last_received_time      \n",
    "            , uniq(user_id) as users_received\n",
    "            , count() as messages_received\n",
    "            , avgIf(time - receiver_prev_time, receiver_prev_time != toDateTime(0)) avg_time_between_messages_received\n",
    "        FROM \n",
    "            user_actions  \n",
    "        GROUP BY \n",
    "            date\n",
    "            , receiver_id\n",
    "    )\n",
    "    SELECT \n",
    "        if(s.user_id != 0, s.date, r.date) as date\n",
    "        , if(s.user_id != 0, s.user_id, r.user_id) as user_id\n",
    "        , s.first_sent_time\n",
    "        , s.last_sent_time\n",
    "        , s.users_sent\n",
    "        , s.messages_sent\n",
    "        , s.avg_time_between_messages_sent\n",
    "        , r.first_received_time\n",
    "        , r.last_received_time\n",
    "        , r.users_received\n",
    "        , r.messages_received\n",
    "        , r.avg_time_between_messages_received  \n",
    "    FROM \n",
    "        senders s\n",
    "        FULL JOIN receivers r ON s.date = r.date AND s.user_id = r.user_id\n",
    "    ORDER BY \n",
    "        date\n",
    "        , user_id            \n",
    "\"\"\"    \n",
    "\n",
    "QUERY_FACT_USER_CONNECTIONS_EXTRACT = \"\"\"\n",
    "    SELECT \n",
    "        toDate(time) as date\n",
    "        , user_id as sender_id\n",
    "        , receiver_id\n",
    "        , count() as messages_count\n",
    "    FROM \n",
    "        message_actions\n",
    "    WHERE \n",
    "        toDate(time) = yesterday()  \n",
    "    GROUP BY \n",
    "        date\n",
    "        , sender_id\n",
    "        , receiver_id\n",
    "    ORDER BY \n",
    "        date\n",
    "        , sender_id\n",
    "        , receiver_id         \n",
    "\"\"\"  \n",
    "\n",
    "QUERY_DIM_USERS_EXTRACT = \"\"\"\n",
    "    SELECT \n",
    "        user_id\n",
    "        , argMax(gender, time) as gender\n",
    "        , argMax(age, time) as age\n",
    "        , argMax(source, time) as source\n",
    "        , argMax(os, time) as os\n",
    "        , argMax(city, time) as city\n",
    "        , argMax(country, time) as country\n",
    "        , argMax(exp_group, time) as exp_group\n",
    "        , toDate(now()) as version\n",
    "    FROM (\n",
    "        SELECT\n",
    "            time\n",
    "            , user_id\n",
    "            , gender\n",
    "            , age\n",
    "            , source\n",
    "            , os \n",
    "            , city \n",
    "            , country\n",
    "            , exp_group \n",
    "        FROM \n",
    "            feed_actions \n",
    "        UNION ALL\n",
    "        SELECT\n",
    "            time\n",
    "            , user_id\n",
    "            , gender\n",
    "            , age\n",
    "            , source\n",
    "            , os \n",
    "            , city \n",
    "            , country\n",
    "            , exp_group       \n",
    "        FROM \n",
    "            message_actions    \n",
    "    )\n",
    "    WHERE \n",
    "        toDate(time) = yesterday()\n",
    "    GROUP BY \n",
    "        user_id\n",
    "    ORDER BY \n",
    "        user_id     \n",
    "\"\"\"  \n",
    "\n",
    "# ==========================================================================\n",
    "# Queries for load\n",
    "# ==========================================================================\n",
    "\n",
    "QUERY_FACT_DAILY_FEED_LOAD = \"\"\"\n",
    "    INSERT INTO fact_daily_feed VALUES    \n",
    "\"\"\"\n",
    "\n",
    "QUERY_FACT_DAILY_POSTS_LOAD = \"\"\"\n",
    "    INSERT INTO fact_daily_posts VALUES      \n",
    "\"\"\"    \n",
    "\n",
    "QUERY_FACT_DAILY_MESSENGER_LOAD = \"\"\"\n",
    "    INSERT INTO fact_daily_messenger VALUES        \n",
    "\"\"\"    \n",
    "\n",
    "QUERY_FACT_USER_CONNECTIONS_LOAD = \"\"\"\n",
    "    INSERT INTO fact_user_connections VALUES\n",
    "\"\"\"  \n",
    "\n",
    "QUERY_DIM_USERS_LOAD = \"\"\"\n",
    "    INSERT INTO dim_users VALUES \n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b08e2d",
   "metadata": {},
   "source": [
    "Create an Airflow DAG that will daily add data from the previous day to the analytical database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analytics_daily_etl.py\n",
    "\"\"\"\n",
    "Analytics Daily ETL DAG\n",
    "\n",
    "This DAG extracts, validates and loads daily analytics data with quality checks\n",
    "from operational database to analytics warehouse.\n",
    "\"\"\"\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.sdk import dag, task\n",
    "from airflow_clickhouse_plugin.hooks.clickhouse import ClickHouseHook\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from textwrap import dedent\n",
    "from elt_queries import (\n",
    "    QUERY_FACT_DAILY_FEED_EXTRACT\n",
    "    , QUERY_FACT_DAILY_POSTS_EXTRACT\n",
    "    , QUERY_FACT_DAILY_MESSENGER_EXTRACT\n",
    "    , QUERY_FACT_USER_CONNECTIONS_EXTRACT\n",
    "    , QUERY_DIM_USERS_EXTRACT\n",
    "    , QUERY_FACT_DAILY_FEED_LOAD\n",
    "    , QUERY_FACT_DAILY_POSTS_LOAD\n",
    "    , QUERY_FACT_DAILY_MESSENGER_LOAD\n",
    "    , QUERY_FACT_USER_CONNECTIONS_LOAD\n",
    "    , QUERY_DIM_USERS_LOAD    \n",
    ")\n",
    "\n",
    "# Database connections\n",
    "SRC_DB_ID = 'ch_src'\n",
    "DST_DB_ID = 'ch_dst'\n",
    "src_hook = ClickHouseHook(clickhouse_conn_id=SRC_DB_ID)\n",
    "dst_hook = ClickHouseHook(clickhouse_conn_id=DST_DB_ID)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ==========================================================================\n",
    "# DAG config\n",
    "# ==========================================================================\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'analytics_team',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2025, 9, 25),\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "}\n",
    "\n",
    "dag_config = {\n",
    "    'default_args': default_args,\n",
    "    'description': 'Daily ETL pipeline for loading data from operational DB to analytics warehouse',\n",
    "    'schedule': '0 3 * * *', # Runs at 3 AM daily\n",
    "    'catchup': False,\n",
    "    'tags': ['analytics', 'etl'],\n",
    "    'max_active_runs': 1,\n",
    "    'doc_md': dedent(\"\"\"\n",
    "    # Analytics Daily ETL with Data Validation\n",
    "    \n",
    "    Extracts, validates and loads daily analytics data with quality checks.\n",
    "    \"\"\")\n",
    "}\n",
    "\n",
    "# ==========================================================================\n",
    "# Constants for validation\n",
    "# ==========================================================================\n",
    "\n",
    "REQUIRED_COLUMNS = {\n",
    "    'daily_feed': [\n",
    "        'date', 'user_id', 'first_view_time', 'first_like_time', \n",
    "        'last_view_time', 'last_like_time', 'posts', 'views', 'likes',\n",
    "        'avg_time_between_views', 'avg_time_between_likes', 'avg_view_to_like_seconds'\n",
    "    ],\n",
    "    'daily_posts': [\n",
    "        'date', 'post_id', 'first_action_time', 'last_action_time', \n",
    "        'views', 'likes', 'unique_viewers', 'unique_likers',\n",
    "        'avg_time_between_views', 'avg_time_between_likes', 'avg_view_to_like_seconds'\n",
    "    ],\n",
    "    'daily_messenger': [\n",
    "        'date', 'user_id', 'first_sent_time', 'last_sent_time', 'users_sent', \n",
    "        'messages_sent', 'avg_time_between_messages_sent', 'first_received_time', \n",
    "        'last_received_time', 'users_received', 'messages_received', \n",
    "        'avg_time_between_messages_received'\n",
    "    ],\n",
    "    'user_connections': ['date', 'sender_id', 'receiver_id', 'messages_count'],\n",
    "    'users': [\n",
    "        'user_id', 'gender', 'age', 'source', 'os', 'city', \n",
    "        'country', 'exp_group', 'version'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ==========================================================================\n",
    "# Helper functions\n",
    "# ==========================================================================\n",
    "\n",
    "def extract_data(query: str, data_type: str) -> pd.DataFrame:\n",
    "    \"\"\"Common extraction logic\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"üöÄ Starting {data_type} extraction...\")\n",
    "        records, column_types = src_hook.execute(query, with_column_types=True)\n",
    "        columns = [col[0] for col in column_types]\n",
    "        df = pd.DataFrame(records, columns=columns)\n",
    "        logger.info(f\"‚úÖ Successfully extracted {len(df)} {data_type} records\")\n",
    "        logger.info(f\"DataFrame shape: {df.shape}, columns: {list(df.columns)}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to extract {data_type} data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def validate_data(df: pd.DataFrame, data_type: str) -> None:\n",
    "    \"\"\"Common validation logic\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"üîç Validating {data_type} data...\")\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(f\"‚ùå {data_type.capitalize()} DataFrame is empty\")\n",
    "        \n",
    "        required_columns = REQUIRED_COLUMNS[data_type]\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"‚ùå Missing required columns in {data_type}: {missing_columns}\")\n",
    "        \n",
    "        logger.info(f\"‚úÖ {data_type.capitalize()} validation passed: {len(df)} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå {data_type.capitalize()} validation failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_data(df: pd.DataFrame, load_query: str, data_type: str) -> None:\n",
    "    \"\"\"Common loading logic\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"üì• Loading {len(df)} {data_type} records...\")\n",
    "        dst_hook.execute(load_query, df.values.tolist())  \n",
    "        logger.info(f\"‚úÖ Successfully loaded {len(df)} {data_type} records\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to load {data_type} records: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def handle_etl_failure(context):\n",
    "    \"\"\"Enhanced error handling for ETL tasks\"\"\"\n",
    "    task_instance = context['task_instance']\n",
    "    exception = context.get('exception')\n",
    "    execution_date = context['execution_date']\n",
    "\n",
    "    logger.error(f\"ETL Task {task_instance.task_id} failed on {execution_date}\")\n",
    "    logger.error(f\"Exception: {str(exception)}\")\n",
    "    logger.error(f\"Task try number: {task_instance.try_number}\")\n",
    "\n",
    "    # Send email notification (uses Airflow's default email config)\n",
    "    try:\n",
    "        task_instance.email_on_failure(subject=f\"ETL Failure: {task_instance.task_id}\", html_content=None)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to send failure email: {e}\")\n",
    "        \n",
    "@dag(**dag_config)\n",
    "def analytics_daily_etl():\n",
    "    # ==========================================================================\n",
    "    # Extract Tasks\n",
    "    # ==========================================================================\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def extract_daily_feed() -> pd.DataFrame:\n",
    "        \"\"\"Extracts daily feed data\"\"\"\n",
    "        return extract_data(QUERY_FACT_DAILY_FEED_EXTRACT, 'daily_feed')\n",
    "    \n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def extract_daily_posts() -> pd.DataFrame:\n",
    "        \"\"\"Extracts daily posts data\"\"\"\n",
    "        return extract_data(QUERY_FACT_DAILY_POSTS_EXTRACT, 'daily_posts')\n",
    "    \n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def extract_daily_messenger() -> pd.DataFrame:\n",
    "        \"\"\"Extracts daily messenger data\"\"\"\n",
    "        return extract_data(QUERY_FACT_DAILY_MESSENGER_EXTRACT, 'daily_messenger')\n",
    "    \n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def extract_user_connections() -> pd.DataFrame:\n",
    "        \"\"\"Extracts user connection data\"\"\"\n",
    "        return extract_data(QUERY_FACT_USER_CONNECTIONS_EXTRACT, 'user_connections')\n",
    "    \n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def extract_users() -> pd.DataFrame:\n",
    "        \"\"\"Extracts users data\"\"\"\n",
    "        return extract_data(QUERY_DIM_USERS_EXTRACT, 'users')\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Validate Tasks\n",
    "    # ==========================================================================\n",
    "    @task(\n",
    "        retries=2,\n",
    "        retry_delay=timedelta(minutes=2),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def validate_daily_feed(df_daily_feed: pd.DataFrame) -> bool:\n",
    "        \"\"\"Validates daily feed data quality\"\"\"\n",
    "        return validate_data(df_daily_feed, 'daily_feed')\n",
    "\n",
    "    @task(\n",
    "        retries=2,\n",
    "        retry_delay=timedelta(minutes=2),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def validate_daily_posts(df_daily_posts: pd.DataFrame) -> bool:\n",
    "        \"\"\"Validates daily posts data quality\"\"\"\n",
    "        return validate_data(df_daily_posts, 'daily_posts')\n",
    "\n",
    "    @task(\n",
    "        retries=2,\n",
    "        retry_delay=timedelta(minutes=2),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def validate_daily_messenger(df_daily_messenger: pd.DataFrame) -> bool:\n",
    "        \"\"\"Validates daily messenger data quality\"\"\"\n",
    "        return validate_data(df_daily_messenger, 'daily_messenger')\n",
    "\n",
    "    @task(\n",
    "        retries=2,\n",
    "        retry_delay=timedelta(minutes=2),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def validate_user_connections(df_user_connections: pd.DataFrame) -> bool:\n",
    "        \"\"\"Validates user connections data quality\"\"\"\n",
    "        return validate_data(df_user_connections, 'user_connections')\n",
    "\n",
    "    @task(\n",
    "        retries=2,\n",
    "        retry_delay=timedelta(minutes=2),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def validate_users(df_users: pd.DataFrame) -> bool:\n",
    "        \"\"\"Validates users dimension data quality\"\"\"\n",
    "        return validate_data(df_users, 'users')\n",
    "        \n",
    "    # ==========================================================================\n",
    "    # Load Tasks\n",
    "    # ==========================================================================\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def load_daily_feed(df_daily_feed: pd.DataFrame) -> None:\n",
    "        \"\"\"Loads daily feed data\"\"\"\n",
    "        load_data(df_daily_feed, QUERY_FACT_DAILY_FEED_LOAD, 'daily_feed')\n",
    "    \n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def load_daily_posts(df_daily_posts: pd.DataFrame) -> None:\n",
    "        \"\"\"Loads daily posts data\"\"\"\n",
    "        load_data(df_daily_posts, QUERY_FACT_DAILY_POSTS_LOAD, 'daily_posts')\n",
    "   \n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def load_daily_messenger(df_daily_messenger: pd.DataFrame) -> None:\n",
    "        \"\"\"Loads daily messenger data\"\"\"\n",
    "        load_data(df_daily_messenger, QUERY_FACT_DAILY_MESSENGER_LOAD, 'daily_messenger')\n",
    "        \n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def load_user_connections(df_user_connections: pd.DataFrame) -> None:\n",
    "        \"\"\"Loads user connections data\"\"\"\n",
    "        load_data(df_user_connections, QUERY_FACT_USER_CONNECTIONS_LOAD, 'user_connections')\n",
    "        \n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def load_users(df_users: pd.DataFrame) -> None:\n",
    "        \"\"\"Loads users data\"\"\"\n",
    "        load_data(df_users, QUERY_DIM_USERS_LOAD, 'users')\n",
    "                                       \n",
    "    # ==========================================================================\n",
    "    # WORKFLOW\n",
    "    # ==========================================================================\n",
    "    # Extract\n",
    "    df_daily_feed = extract_daily_feed()\n",
    "    df_daily_posts = extract_daily_posts()\n",
    "    df_daily_messenger = extract_daily_messenger()\n",
    "    df_user_connections = extract_user_connections()\n",
    "    df_users = extract_users()\n",
    "\n",
    "    # Validate\n",
    "    feed_valid = validate_daily_feed(df_daily_feed)\n",
    "    posts_valid = validate_daily_posts(df_daily_posts)\n",
    "    messenger_valid = validate_daily_messenger(df_daily_messenger)\n",
    "    connections_valid = validate_user_connections(df_user_connections)\n",
    "    users_valid = validate_users(df_users)\n",
    "\n",
    "    # Load facts\n",
    "    load_daily_feed_task = load_daily_feed(df_daily_feed)\n",
    "    load_daily_posts_task = load_daily_posts(df_daily_posts)\n",
    "    load_daily_messenger_task = load_daily_messenger(df_daily_messenger)\n",
    "    load_user_connections_task = load_user_connections(df_user_connections)\n",
    "    \n",
    "    # Load dimensions\n",
    "    load_users_task = load_users(df_users)\n",
    "\n",
    "    # Tasks dependences\n",
    "    feed_valid >> load_daily_feed_task\n",
    "    posts_valid >> load_daily_posts_task\n",
    "    messenger_valid >> load_daily_messenger_task\n",
    "    connections_valid >> load_user_connections_task\n",
    "    users_valid >> load_users_task\n",
    "    \n",
    "# ==========================================================================\n",
    "# Instantiate the DAG\n",
    "# ==========================================================================\n",
    "analytics_daily_etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5ea307",
   "metadata": {},
   "source": [
    "# General Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e9f5a3",
   "metadata": {},
   "source": [
    "- **Star Schema Designed:** Developed a star schema for the analytical database\n",
    "- **Metric Queries Created:** Built queries to the product database for calculating required metrics for the analytical database\n",
    "- **Star Schema Tables Implemented:** Created all necessary tables for the star schema in the analytical database\n",
    "- **Data Migration Completed:** Extracted required data from the product database and loaded it into the analytical database\n",
    "- **Dashboard Optimization:** Created materialized views to optimize dashboard performance\n",
    "- **Automation Pipeline Built:** Developed Airflow DAG for daily incremental data loading for yesterday's data\n",
    "- **Goal Achieved:** Established analytical database and ETL process for continuous data updates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pagri-projects-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
