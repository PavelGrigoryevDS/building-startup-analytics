{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6b911e",
   "metadata": {},
   "source": [
    "# 7. Alert System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1176968c",
   "metadata": {},
   "source": [
    "**Context:**\n",
    "\n",
    "- To proactively identify issues before they impact users, we need a system that continuously monitors key product metrics and alerts the team about anomalies. \n",
    "- This project implements an automated alert system that checks critical metrics every 15 minutes and sends immediate notifications when deviations are detected.\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "- To build a robust anomaly detection system that monitors key product metrics in real-time, automatically alerts the team via Telegram when anomalies occur, and provides actionable insights for rapid investigation.\n",
    "   \n",
    "**Data Sources:**\n",
    "\n",
    "- `feed_actions` - News feed activity\n",
    "- `message_actions` - Messaging activity \n",
    "\n",
    "**Key Achievements:**\n",
    "\n",
    "- **SQL Queries Developed:** \n",
    "  - Created optimized queries to extract and calculate key metrics from ClickHouse database\n",
    "- **Airflow DAG Implemented:** \n",
    "  - Built automated pipeline for continuous 15-minute monitoring and alert generation  \n",
    "- **Anomaly Detection Engine:** \n",
    "  - Successfully implemented MAD-based statistical method for identifying metric deviations\n",
    "- **Configurable System:** \n",
    "  - Built flexible detection with adjustable sensitivity thresholds\n",
    "- **Real-time Monitoring:** \n",
    "  - Established 24/7 tracking of feed and messenger metrics (DAU, views, likes, CTR, messages)\n",
    "- **Automated Alert System:** \n",
    "  - Deployed Telegram integration for immediate notification delivery\n",
    "\n",
    "**Business Impact:**\n",
    "\n",
    "- Established proactive monitoring system that enables rapid response to critical issues before they escalate to affect user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f869553",
   "metadata": {},
   "source": [
    "## 7.1 Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c1c68",
   "metadata": {},
   "source": [
    "In the product database on ClickHouse, the data is stored in the following tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95466e",
   "metadata": {},
   "source": [
    "Table feed_actions\n",
    "\n",
    "Field | Description\n",
    "-|-\n",
    "user_id | User ID\n",
    "post_id | Post ID\n",
    "action | Action: view or like\n",
    "time | Timestamp\n",
    "gender | User's gender\n",
    "age | User's age (1 = Male)\n",
    "os | User's OS\n",
    "source | Traffic source\n",
    "country | User's country\n",
    "city | User's city\n",
    "exp_group | A/B test group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe3461",
   "metadata": {},
   "source": [
    "Table message_actions\n",
    "\n",
    "Field | Description\n",
    "-|-\n",
    "user_id | Sender's ID\n",
    "receiver_id | Receiver's ID\n",
    "time | Send timestamp\n",
    "gender | Sender's gender\n",
    "age | Sender's age (1 = Male)\n",
    "os | Sender's OS\n",
    "source | Sender's traffic source\n",
    "country | Sender's country\n",
    "city | Sender's city\n",
    "exp_group | Sender's A/B test group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d8934",
   "metadata": {},
   "source": [
    "## 7.2 Anomaly Detection Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b0a6a",
   "metadata": {},
   "source": [
    "- **Statistical Method:** \n",
    "    - Median Absolute Deviation (MAD) for robust outlier detection resistant to extreme values\n",
    "- **Time-based Sampling:** \n",
    "  - Uses the same 15-minute interval from each of the previous 30 days (e.g., 14:15-14:30 from each day)\n",
    "- **Dynamic Baseline:** \n",
    "  - For each monitoring cycle, calculates median and MAD from 30 historical same-time-interval data points\n",
    "- **Bound Calculation:** \n",
    "  - Upper and lower bounds derived as: rolling_median ± threshold × (MAD × 1.4826)\n",
    "- **Bound Smoothing:** \n",
    "  - 3-point moving average applied to bounds to reduce noise while maintaining sensitivity\n",
    "- **Anomaly Trigger:** \n",
    "  - Current data point is flagged as anomalous when it falls outside the calculated upper or lower bounds\n",
    "- **Continuous Adaptation:** \n",
    "  - Bounds automatically recalculate for each new 15-minute interval using updated 30-day window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d6626",
   "metadata": {},
   "source": [
    "## 7.3 Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b92df38",
   "metadata": {},
   "source": [
    "Create function for calculate anomaly bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_anomaly_bounds_mad(\n",
    "    column: pd.Series, \n",
    "    window_size: int = 30,\n",
    "    threshold: float = 3.0,\n",
    "    smooth_bounds: bool = True,\n",
    "    smooth_window: int = 3 \n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate anomaly bounds using Median Absolute Deviation (MAD) via rolling.\n",
    "    \n",
    "    Args:\n",
    "        column: Time series data with datetime index\n",
    "        window_size: Number of previous points for bounds calculation  \n",
    "        threshold: MAD multiplier (default: 3.0 ~ 99.7% for normal distribution)\n",
    "        smooth_bounds: Whether to smooth bounds (default: True)  \n",
    "        smooth_window: Window size for smoothing (default: 3)\n",
    "    \"\"\"\n",
    "    shifted = column.shift(1)\n",
    "    rolling_median = shifted.rolling(window_size).median()\n",
    "    rolling_mad = (\n",
    "        shifted.rolling(window_size)\n",
    "        .apply(lambda x: stats.median_abs_deviation(x, nan_policy=\"omit\"))\n",
    "    )\n",
    "    # Converting MAD to \"standard deviation\"\n",
    "    mad_std = rolling_mad * 1.4826\n",
    "    \n",
    "    lower_bounds = rolling_median - threshold * mad_std\n",
    "    upper_bounds = rolling_median + threshold * mad_std\n",
    "    if smooth_bounds:\n",
    "        lower_bounds = lower_bounds.rolling(window=smooth_window, min_periods=1).mean()\n",
    "        upper_bounds = upper_bounds.rolling(window=smooth_window, min_periods=1).mean()\n",
    "    \n",
    "    df_bounds = pd.DataFrame({\n",
    "        'value': column,\n",
    "        'lower_bound': lower_bounds,\n",
    "        'upper_bound': upper_bounds\n",
    "    }, index=column.index)\n",
    "    df_bounds = df_bounds.replace([np.inf, -np.inf], np.nan)\n",
    "    return df_bounds.tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd072f3",
   "metadata": {},
   "source": [
    "Create a function to build a chart with a highlighted anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f095ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anomaly_chart(\n",
    "    bounds_df: pd.DataFrame\n",
    "    , metric_name: str\n",
    "    ) -> go.Figure:\n",
    "    \"\"\"Create subplots for detected anomaly\"\"\"\n",
    "    \n",
    "    historical_series = bounds_df['value']\n",
    "    current_value = bounds_df['value'].iloc[-1]\n",
    "    lower_bounds = bounds_df['lower_bound']\n",
    "    upper_bounds = bounds_df['upper_bound']\n",
    "        \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\n",
    "            f'Distribution',\n",
    "            f'Historical Trend with Bounds'\n",
    "        )\n",
    "        , horizontal_spacing=0\n",
    "        , column_widths=[0.4, 0.6]\n",
    "        , shared_yaxes=True\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Violin(\n",
    "            y=historical_series.values,\n",
    "            box_visible=True,\n",
    "            meanline_visible=True,\n",
    "            points=False,\n",
    "            name=metric_name,\n",
    "            marker=dict(color='gray'),\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[metric_name], y=[current_value],\n",
    "            mode='markers',\n",
    "            marker=dict(color='red', size=12, symbol='x'),\n",
    "            name='Current Value',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=historical_series.index,\n",
    "            y=historical_series.values,\n",
    "            mode='lines+markers',\n",
    "            name='Historical data',\n",
    "            line=dict(color='gray', shape='spline'),\n",
    "            marker=dict(size=7)\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=upper_bounds.index,\n",
    "            y=upper_bounds.values,\n",
    "            mode='lines',\n",
    "            name='Upper Bound',\n",
    "            line=dict(color='gray', width=1, dash='dash'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=lower_bounds.index,\n",
    "            y=lower_bounds.values,\n",
    "            mode='lines',\n",
    "            name='Lower Bound',\n",
    "            line=dict(color='gray', width=1, dash='dash'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[historical_series.index.max()],\n",
    "            y=[current_value],\n",
    "            mode='markers',\n",
    "            marker=dict(color='red', size=10, symbol='x'),\n",
    "            name='Current Value',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Anomaly Detection: {metric_name}\",\n",
    "        height=450,\n",
    "        width=1000,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(title_text=metric_name, row=1, col=1)\n",
    "    fig.update_yaxes(title_text=None, row=1, col=2)\n",
    "    fig.update_xaxes(visible=False, row=1, col=1)\n",
    "    fig.update_xaxes(title='Date', tickformat='%b %d', row=1, col=2)\n",
    "    \n",
    "    return fig    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c8bb7",
   "metadata": {},
   "source": [
    "## 7.4 Airflow DAG Implementation  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453ebfd",
   "metadata": {},
   "source": [
    "Complete DAG code for anomalies detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecd95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.operators.python import get_current_context\n",
    "from textwrap import dedent\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "env_path = os.path.join(current_dir, '.env')\n",
    "sys.path.insert(0, current_dir)\n",
    "from utils_for_dags import (\n",
    "    ChConnector\n",
    "    , TelegramBot\n",
    "    , create_anomaly_chart\n",
    "    , calculate_anomaly_bounds_mad\n",
    ")\n",
    "load_dotenv(env_path)\n",
    "CHAT_ID_ALERT = os.getenv('CHAT_ID_ALERT')\n",
    "db = ChConnector()\n",
    "bot = TelegramBot(chat_id=CHAT_ID_ALERT)\n",
    "\n",
    "# Configure logger\n",
    "logger = logging.getLogger('alert_system')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Pavel Grigoryev',\n",
    "    'depends_on_past': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'start_date': datetime(2025, 9, 25),\n",
    "}\n",
    "\n",
    "dag_config = {\n",
    "    'default_args': default_args,\n",
    "    'description': 'DAG for detecting anomalies and sending report in Telegram',\n",
    "    'schedule_interval': '*/15 * * * *', # Every 15 minutes\n",
    "    'catchup': False,\n",
    "    'tags': ['anomalies'],\n",
    "    'max_active_runs': 1,\n",
    "}\n",
    "\n",
    "QUERY = '''\n",
    "    WITH feed_metrics AS (\n",
    "        SELECT\n",
    "            toStartOfFifteenMinutes(time) AS start_of_15min\n",
    "            , uniqExact(user_id) AS users_feed\n",
    "            , countIf(action = 'view') AS views\n",
    "            , countIf(action = 'like') AS likes\n",
    "            , likes / views as ctr\n",
    "        FROM \n",
    "            feed_actions\n",
    "        WHERE \n",
    "            toDate(time) >= toDate(now()) - 59\n",
    "            AND toHour(time) = toHour(now() - INTERVAL 15 MINUTE)\n",
    "            AND toMinute(time) >= toMinute(toStartOfFifteenMinutes(now() - INTERVAL 15 MINUTE))\n",
    "            AND toMinute(time) < toMinute(toStartOfFifteenMinutes(now() - INTERVAL 15 MINUTE)) + 15\n",
    "        GROUP BY \n",
    "            start_of_15min\n",
    "    )\n",
    "    , messenger_metrics AS (\n",
    "        SELECT\n",
    "            toStartOfFifteenMinutes(time) AS start_of_15min\n",
    "            , uniqExact(user_id) AS users_messenger\n",
    "            , count() AS messages\n",
    "        FROM \n",
    "            message_actions\n",
    "        WHERE \n",
    "            toDate(time) >= toDate(now()) - 59\n",
    "            AND toHour(time) = toHour(now() - INTERVAL 15 MINUTE)\n",
    "            AND toMinute(time) >= toMinute(toStartOfFifteenMinutes(now() - INTERVAL 15 MINUTE))\n",
    "            AND toMinute(time) < toMinute(toStartOfFifteenMinutes(now() - INTERVAL 15 MINUTE)) + 15\n",
    "        GROUP BY \n",
    "            start_of_15min\n",
    "    )\n",
    "    SELECT\n",
    "        if(f.start_of_15min != toDate(0), f.start_of_15min, m.start_of_15min) AS start_of_15min\n",
    "        , f.users_feed\n",
    "        , m.users_messenger\n",
    "        , f.views\n",
    "        , f.likes\n",
    "        , f.likes / nullIf(f.views, 0) AS ctr\n",
    "        , m.messages\n",
    "    FROM \n",
    "        feed_metrics f\n",
    "        FULL OUTER JOIN messenger_metrics m ON f.start_of_15min = m.start_of_15min\n",
    "    ORDER BY \n",
    "        start_of_15min\n",
    "'''\n",
    "\n",
    "METRIC_CONFIG = {\n",
    "    'users_feed': {\n",
    "        'name': 'Feed Users',\n",
    "        'threshold': 4.5,  \n",
    "        'window_size': 30,\n",
    "        'smooth_bounds': True,\n",
    "        'smooth_window': 3,\n",
    "        'format': '{:.0f}',  \n",
    "    },\n",
    "    'users_messenger': {\n",
    "        'name': 'Messenger Users', \n",
    "        'threshold': 4.5,\n",
    "        'window_size': 30,\n",
    "        'smooth_bounds': True,\n",
    "        'smooth_window': 3,\n",
    "        'format': '{:.0f}',\n",
    "    },\n",
    "    'views': {\n",
    "        'name': 'Post Views',\n",
    "        'threshold': 4.0,\n",
    "        'window_size': 30,\n",
    "        'smooth_bounds': True, \n",
    "        'smooth_window': 3,\n",
    "        'format': '{:.0f}',\n",
    "    },\n",
    "    'likes': {\n",
    "        'name': 'Post Likes',\n",
    "        'threshold': 4.0,\n",
    "        'window_size': 30,\n",
    "        'smooth_bounds': True,\n",
    "        'smooth_window': 3,\n",
    "        'format': '{:.0f}',\n",
    "    },\n",
    "    'ctr': {\n",
    "        'name': 'CTR',\n",
    "        'threshold': 4.0,  \n",
    "        'window_size': 30,\n",
    "        'smooth_bounds': True,\n",
    "        'smooth_window': 3,\n",
    "        'format': '{:.2%}', \n",
    "    },\n",
    "    'messages': {\n",
    "        'name': 'Messages Sent',\n",
    "        'threshold': 4.0, \n",
    "        'window_size': 30,\n",
    "        'smooth_bounds': True,\n",
    "        'smooth_window': 3,\n",
    "        'format': '{:.0f}',\n",
    "    }\n",
    "}\n",
    "\n",
    "def handle_failure(context):\n",
    "    \"\"\"\n",
    "    Callback function for processing unsuccessful tasks\n",
    "    \"\"\"\n",
    "    exception = context.get('exception')\n",
    "    task_instance = context['task_instance']\n",
    "\n",
    "    logger.error(f\"Task {task_instance.task_id} failed:\")\n",
    "    logger.error(f\"Error: {exception}\")\n",
    "    logger.error(f\"Execution date: {context['ds']}\")\n",
    "    logger.error(f\"Attempt: {context['ti'].try_number}\")\n",
    "\n",
    "@dag(**dag_config)\n",
    "def alert_system():\n",
    "    \"\"\"\n",
    "    DAG runs every 15 minutes, extracts data from database, \n",
    "    checks for anomalies, and sends alerts to Telegram if any detected.\n",
    "    \"\"\"\n",
    "    # ==========================================================================\n",
    "    # Extract\n",
    "    # ==========================================================================    \n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_failure\n",
    "    )\n",
    "    def extract() -> pd.DataFrame:\n",
    "        \"\"\"Extracts metrics data from database\"\"\"\n",
    "        return (\n",
    "            db.get_df(query=QUERY)\n",
    "            .set_index('start_of_15min')\n",
    "        )\n",
    "            \n",
    "    # ==========================================================================\n",
    "    # Transform\n",
    "    # ==========================================================================\n",
    "\n",
    "    def make_calculate_bounds_task(metric: str):\n",
    "        @task(\n",
    "            task_id=f'calculate_{metric}_bounds',\n",
    "            retries=3,\n",
    "            retry_delay=timedelta(minutes=5),\n",
    "            on_failure_callback=handle_failure\n",
    "        )\n",
    "        def calculate_single_bounds(df: pd.DataFrame) -> pd.DataFrame:\n",
    "            \"\"\"Calculate bounds for single metric\"\"\"\n",
    "            logger.info(f\"📊 Calculating bounds for {metric}\")\n",
    "            config = METRIC_CONFIG[metric]\n",
    "            return calculate_anomaly_bounds_mad(\n",
    "                df[metric],\n",
    "                threshold=config['threshold'],\n",
    "                window_size=config['window_size'],\n",
    "                smooth_bounds=config['smooth_bounds'],\n",
    "                smooth_window=config['smooth_window']\n",
    "            )\n",
    "        return calculate_single_bounds\n",
    "    \n",
    "    def make_metric_alert_task(metric: str):\n",
    "        @task(\n",
    "            task_id=f'create_{metric}_alert',\n",
    "            retries=3,\n",
    "            retry_delay=timedelta(minutes=5),\n",
    "            on_failure_callback=handle_failure,\n",
    "            multiple_outputs=True\n",
    "        )\n",
    "        def create_alert_content(df_bounds: pd.DataFrame) -> dict:    \n",
    "            \"\"\"Creates alert message and chart for a specific metric\"\"\"\n",
    "            logger.info(f\"📊 Creating alert content for {metric}\")\n",
    "            config = METRIC_CONFIG[metric] \n",
    "            value_format = config.get('format', '{:.0f}')\n",
    "            last_timestamp = df_bounds.index[-1]\n",
    "            last_row = df_bounds.iloc[-1]\n",
    "            last_value = last_row['value']\n",
    "            previous_value = df_bounds.iloc[-2]['value']\n",
    "            last_lower_bound = last_row['lower_bound']\n",
    "            last_upper_bound = last_row['upper_bound']\n",
    "            metric_name = config['name']\n",
    "\n",
    "            is_anomaly = not (last_lower_bound <= last_value <= last_upper_bound)\n",
    "            \n",
    "            if is_anomaly:\n",
    "                logger.warning(f\"🚨 Anomaly detected in {metric_name}\")\n",
    "                \n",
    "                if last_value < last_lower_bound:\n",
    "                    if last_lower_bound > 0:\n",
    "                        deviation_from_bound = ((last_lower_bound - last_value) / last_lower_bound) * 100\n",
    "                    else:   \n",
    "                        deviation_from_bound = 100.0    \n",
    "                    direction = \"below\"\n",
    "                else:  \n",
    "                    if last_upper_bound > 0:\n",
    "                        deviation_from_bound = ((last_value - last_upper_bound) / last_upper_bound) * 100\n",
    "                    else:\n",
    "                        deviation_from_bound = 100.0                        \n",
    "                    direction = \"above\" \n",
    "                \n",
    "                if previous_value > 0:\n",
    "                    daily_change = ((last_value - previous_value) / previous_value) * 100\n",
    "                    deviation_from_yesterday = f\"Change from yesterday: {daily_change:+.1f}%\"    \n",
    "                else:\n",
    "                    deviation_from_yesterday = \"Change from yesterday: N/A (no previous data)\"\n",
    "                time_slot = f\"{last_timestamp.strftime('%H:%M')} - {(last_timestamp + pd.Timedelta(minutes=15)).strftime('%H:%M')}\"                     \n",
    "                # Create alert message\n",
    "                msg = dedent(f\"\"\"\n",
    "                    🚨 Anomaly Alert\n",
    "                    \n",
    "                    ⏰ Time slot: {time_slot}\n",
    "                    📊 Metric: {metric_name}\n",
    "                    🔢 Current value: {value_format.format(last_value)} \n",
    "                    ⚠️ Deviation: {deviation_from_bound:.1f}% {direction} range\n",
    "                    📅 {deviation_from_yesterday}\n",
    "                    ↔️ Expected range: {value_format.format(last_lower_bound)} - {value_format.format(last_upper_bound)}\n",
    "                    📈 [Go To Dash](https://superset.lab.karpov.courses/superset/dashboard/7569/)\n",
    "                    🕵️ @alert_jedis @avengers_analytics\n",
    "                \"\"\")\n",
    "                \n",
    "                # Create chart\n",
    "                fig = create_anomaly_chart(\n",
    "                    bounds_df=df_bounds,\n",
    "                    metric_name=metric_name\n",
    "                )\n",
    "                return {\"message\": msg, \"chart\": fig, \"has_anomaly\": True}\n",
    "            \n",
    "            logger.info(f\"✅ No anomaly in {metric_name}\")\n",
    "            \n",
    "            return {\"message\": None, \"chart\": None, \"has_anomaly\": False}\n",
    "        return create_alert_content\n",
    "\n",
    "    def make_send_message_task(metric: str):\n",
    "        @task(\n",
    "            task_id=f'send_{metric}_message',\n",
    "            retries=3,\n",
    "            retry_delay=timedelta(minutes=5),\n",
    "            on_failure_callback=handle_failure\n",
    "        )\n",
    "        def send_alert_message(message: str) -> None:\n",
    "            \"\"\"Sends alert message for specific metric\"\"\"\n",
    "            if message:\n",
    "                logger.info(f\"📨 Sending {metric} alert message...\")\n",
    "                if not bot.send_message(message=message):\n",
    "                    logger.error(f\"❌ Failed to send {metric} message\")\n",
    "                    raise Exception(f\"Failed to send {metric} message\")\n",
    "                logger.info(f\"✅ {metric} message sent successfully\")\n",
    "        return send_alert_message\n",
    "\n",
    "    def make_send_chart_task(metric: str):\n",
    "        @task(\n",
    "            task_id=f'send_{metric}_chart',\n",
    "            retries=3,\n",
    "            retry_delay=timedelta(minutes=5),\n",
    "            on_failure_callback=handle_failure\n",
    "        )\n",
    "        def send_alert_chart(figure: go.Figure) -> None:\n",
    "            \"\"\"Sends alert chart for specific metric\"\"\"\n",
    "            if figure:\n",
    "                logger.info(f\"📊 Sending {metric} chart...\")\n",
    "                if not bot.send_chart(figure=figure):\n",
    "                    logger.error(f\"❌ Failed to send {metric} chart\")\n",
    "                    raise Exception(f\"Failed to send {metric} chart\")\n",
    "                logger.info(f\"✅ {metric} chart sent successfully\")\n",
    "        return send_alert_chart\n",
    "\n",
    "    # ==========================================================================\n",
    "    # WORKFLOW\n",
    "    # ==========================================================================\n",
    "    \n",
    "    # Extract data\n",
    "    df = extract()\n",
    "\n",
    "    # Creating a chain of dependencies between graphs\n",
    "    previous_chart_task = None\n",
    "\n",
    "    # Create and send alerts for each metric\n",
    "    for metric in METRIC_CONFIG.keys():\n",
    "            \n",
    "        # Create task instances for this metric\n",
    "        bounds_task = make_calculate_bounds_task(metric)\n",
    "        alert_task = make_metric_alert_task(metric)\n",
    "        send_message_task = make_send_message_task(metric)\n",
    "        send_chart_task = make_send_chart_task(metric)\n",
    "        \n",
    "        # Execute workflow for this metric\n",
    "        df_bounds = bounds_task(df)\n",
    "        alert_content = alert_task(df_bounds)\n",
    "        send_message = send_message_task(alert_content['message'])\n",
    "        send_chart = send_chart_task(alert_content['chart'])\n",
    "        \n",
    "        # Execution order\n",
    "        send_message >> send_chart\n",
    "        \n",
    "        # Between metrics: graphs are sent sequentially\n",
    "        if previous_chart_task:\n",
    "            previous_chart_task >> send_chart\n",
    "            \n",
    "        previous_chart_task = send_chart\n",
    "\n",
    "# Initialize DAG\n",
    "alert_system = alert_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025cdb7",
   "metadata": {},
   "source": [
    "## 7.5 Anomaly Alert Screenshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f000afd1",
   "metadata": {},
   "source": [
    "Below are screenshots of a sample anomaly alert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e50ad",
   "metadata": {},
   "source": [
    "<img src=\"anomaly_detection_part_1.jpg\" width=\"500\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561a158",
   "metadata": {},
   "source": [
    "<img src=\"anomaly_detection_part_2.jpg\" width=\"500\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce3cfc",
   "metadata": {},
   "source": [
    "## 7.6 Conclusion\n",
    "\n",
    "- **SQL Queries Developed:** \n",
    "  - Created optimized queries to extract and calculate key metrics from ClickHouse database\n",
    "- **Airflow DAG Implemented:** \n",
    "  - Built automated pipeline for continuous 15-minute monitoring and alert generation  \n",
    "- **Anomaly Detection Engine:** \n",
    "  - Successfully implemented MAD-based statistical method for identifying metric deviations\n",
    "- **Configurable System:** \n",
    "  - Built flexible detection with adjustable sensitivity thresholds\n",
    "- **Real-time Monitoring:** \n",
    "  - Established 24/7 tracking of feed and messenger metrics (DAU, views, likes, CTR, messages)\n",
    "- **Automated Alert System:** \n",
    "  - Deployed Telegram integration for immediate notification delivery"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
